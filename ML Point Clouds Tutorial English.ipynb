{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical: Artificial Intelligence in Python\n",
    "\n",
    "The objective of this tutorial is to familiarize you with Artificial Intelligence in a case of semantic segmentation of point clouds.The point clouds used were acquired with Velodyne64 and are available from SemanticKITTI.\n",
    "\n",
    "A semantic segmentation problem consists of classifying each basic element that makes up the data. In image, it is pixels, in point clouds, it is points. Sometimes semantic segmentation is also called pixel-by-pixel or point-by-point classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives\n",
    "\n",
    "In this tutorial we will look at code snippets that explain how to prepare data for an AI algorithm, train it and use it to make predictions. The operations that will be studied next are:\n",
    "\n",
    "- Reading and writing point clouds as txt data.\n",
    "- Data preparation for Artificial Intelligence\n",
    "- Feature extraction\n",
    "- Algorithm training\n",
    "- Analysis of the results using metrics: confusion, precision, recall, f1-score and accuracy matrices\n",
    "- Classification visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries\n",
    "\n",
    "The algorithms used in artificial intelligence are quite complex. Although we can program an algorithm with simple knowledge of the subject, getting it to reach the hit rates of algorithms already developed by other authors requires a strong background in mathematics and computer science, as well as powerful test servers. Fortunately, most AI algorithms are open source and compiled in libraries.\n",
    "\n",
    "In this tutorial we will use the *pyntcloud*, *scikit-learn* and *numpy* libraries.\n",
    "\n",
    "The first task is to install these libraries in our environment. Once the library is installed, we are going to import all the functions we need. If any of them give error, check that the librerias are correctly installed in the environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyntcloud import PyntCloud\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading and writing of point clouds as txt data\n",
    "\n",
    "AI-based algorithms need a training phase before they can be used to classify new data. As input data we are going to use the point clouds 000036 (train) and 000079 (test). We are  going to load point clouds as txt, since this type of format, together with csv, is the most common in which we can find other types of data for AI. The txt cloud is structured in 1 point per row and 1 attribute per column, and ' ' is specified as delimiter between columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data reading\n",
    "train_data = np.loadtxt(\"Nubes/000036.txt\", delimiter=' ')\n",
    "\n",
    "# Visualization\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The organization of data in AI is distributed in matrices where each row corresponds to a sample and each column to an attribute. This is very similar to how point clouds are distributed. In the above data we can see that each point contains 3 coordinates and the fourth column corresponds to the label according to the following code:\n",
    "- 1: car\n",
    "- 2: building\n",
    "- 3: ground\n",
    "- 4: vegetation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation for Artificial Intelligence\n",
    "\n",
    "But this data cannot be used directly in an AI algorithm, it must be divided into an NxM attribute matrix, where N is the number of samples and M is the number of attributes, and a label matrix (Nx1).\n",
    "\n",
    "Furthermore, in point clouds, coordinates cannot be used as training attributes for AI, since the position of points in absolute coordinates does not represent or relate to new data. In point clouds, coordinates are used to extract new geometric features that are suitable for the classifier.\n",
    "\n",
    "Therefore, first of all, let's divide the input matrix into two matrices:\n",
    "- Coordinate matrix (defined as a column-titled dataframe, necessary for its transformation to Pyntcloud object).\n",
    "- Label array (defined as numpy array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coordinate matrix\n",
    "coord = pd.DataFrame(list(zip(train_data[:,0],train_data[:,1],train_data[:,2])))  \n",
    "\n",
    "# Assign title to columns\n",
    "coord.columns =['x', 'y', 'z']\n",
    "\n",
    "# Visualization\n",
    "print(coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract label matrix\n",
    "train_labels = train_data[:,3]\n",
    "\n",
    "# Visualization\n",
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction\n",
    "\n",
    "As mentioned above, coordinates are not useful for training an AI-based algorithm, and therefore we have to extract geometric features. Although we can compute them using the matrix directly, it is easier to convert the matrix to a cloud object in the pyntcloud library and use its functions to compute them. \n",
    "\n",
    "For a correct conversion, cloud does not support a numpyarray, but a dataframe with the titles \"x\", \"y\", and \"z\", calculated in the previous step, has to be used.\n",
    "\n",
    "The features used for training will be:\n",
    "- Normals\n",
    "- Curvature\n",
    "- Omnivariate\n",
    "- Linearity\n",
    "- Planarity\n",
    "- Scattering\n",
    "\n",
    "The use of these characteristics is widely extended in point clouds, their calculation is explained in the following scientific paper:\n",
    "- Weinmann, M., Jutzi, B., & Mallet, C. (2014). Semantic 3D scene interpretation: A framework combining optimal neighborhood size selection with relevant features. ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences, 2(3), 181."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion\n",
    "cloud = PyntCloud(coord)\n",
    "\n",
    "# Calculation of 25 neighbors\n",
    "k_neighbors_25 = cloud.get_neighbors(k=25)\n",
    "\n",
    "# Normal estimation\n",
    "cloud.add_scalar_field(\"normals\", k_neighbors=k_neighbors_25)\n",
    "\n",
    "# Visualization\n",
    "print(cloud.points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will compute the characteristics based on eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation of eigenvalues\n",
    "eigenvalues = cloud.add_scalar_field(\"eigen_values\", k_neighbors=k_neighbors_25)\n",
    "\n",
    "# Calculation of local geometric features\n",
    "cloud.add_scalar_field(\"curvature\", ev=eigenvalues)\n",
    "cloud.add_scalar_field(\"omnivariance\", ev=eigenvalues)\n",
    "cloud.add_scalar_field(\"linearity\", ev=eigenvalues)\n",
    "cloud.add_scalar_field(\"planarity\", ev=eigenvalues)\n",
    "cloud.add_scalar_field(\"sphericity\", ev=eigenvalues)\n",
    "\n",
    "# Visualization\n",
    "print(cloud.points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be used in the algorithm, the features must be returned in numpy array format. Also, not all \"points\" features are useful, we will select the normals and those calculated from the eigenvalues, but not the coordinates or the eigenvalues themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection\n",
    "train_features = cloud.points[['nx(26)','ny(26)','nz(26)','curvature(26)','omnivariance(26)','linearity(26)','planarity(26)','sphericity(26)',]].to_numpy()\n",
    "\n",
    "# Visualization\n",
    "print(train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions\n",
    "\n",
    "Before proceeding, let's define some functions.\n",
    "\n",
    "We have seen step by step how to read the input data, split it and extract its features. These features are the ones used for training, but they are also necessary for future classification, so it is necessary for each sample to extract these features and generate a data matrix whose attributes correspond to the training order. To avoid repeating code each time we load a data matrix, we will define two functions. The first one will separate the data. The second one will extract the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separar_input(input_matrix):\n",
    "    coord = pd.DataFrame(list(zip(input_matrix[:,0],input_matrix[:,1],input_matrix[:,2])))  \n",
    "    coord.columns =['x', 'y', 'z']\n",
    "    labels = input_matrix[:,3]\n",
    "    return coord, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_features(coord):\n",
    "    # Create point cloud\n",
    "    cloud = PyntCloud(coord)\n",
    "    # Calculate neighbors\n",
    "    k_neighbors_25 = cloud.get_neighbors(k=25)\n",
    "    # Calculate and add normals\n",
    "    cloud.add_scalar_field(\"normals\", k_neighbors=k_neighbors_25)\n",
    "    # Calculate and add eigenvalues\n",
    "    eigenvalues = cloud.add_scalar_field(\"eigen_values\", k_neighbors=k_neighbors_25)\n",
    "    # Calculate and add other geometrical characteristics\n",
    "    cloud.add_scalar_field(\"curvature\", ev=eigenvalues)\n",
    "    cloud.add_scalar_field(\"omnivariance\", ev=eigenvalues)\n",
    "    cloud.add_scalar_field(\"linearity\", ev=eigenvalues)\n",
    "    cloud.add_scalar_field(\"planarity\", ev=eigenvalues)\n",
    "    cloud.add_scalar_field(\"sphericity\", ev=eigenvalues)\n",
    "    # Transform point dataframe to feature nparray (in order)\n",
    "    features = cloud.points[['nx(26)','ny(26)','nz(26)','curvature(26)','omnivariance(26)','linearity(26)','planarity(26)','sphericity(26)',]].to_numpy()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate input data\n",
    "train_coord,train_labels = separar_input(train_data)\n",
    "\n",
    "# Extract features\n",
    "train_features = extraer_features(train_coord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm training\n",
    "\n",
    "In this step we get to the core of the algorithm. Once all the data are prepared we proceed to the training. In this practice we are going to train and use the two most used algorithms nowadays: \n",
    "- Support Vector Machine. Given a set of points, in which each of them belongs to one of two possible categories, an SVM-based algorithm builds a model capable of predicting whether a new point (whose category we do not know) belongs to one category or the other.\n",
    "\n",
    "<center> <img src=\"img/svm.png\"></center>\n",
    "<center>Fuente: https://en.wikipedia.org/wiki/Support-vector_machine</center>\n",
    "\n",
    "- Random Forest. It is a combination of predictor trees such that each tree depends on the values of a random vector tested independently and with the same distribution for each of them. The result of the classification is the class predicted by the majority of all trees.\n",
    "\n",
    "<center> <img src=\"img/rf.png\"></center>\n",
    "<center>Fuente: https://es.wikipedia.org/wiki/Random_forest</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all the work has been done, all you have to do is give the data to the algorithm to train.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define SVM classifier\n",
    "clf_svm = svm.SVC()\n",
    "\n",
    "# Train (this may take several minutes)\n",
    "clf_svm.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RF classifier\n",
    "clf_rf = RandomForestClassifier()\n",
    "\n",
    "#Train (this should go faster than the previous one)\n",
    "clf_rf.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of results through metrics\n",
    "\n",
    "Metrics let us know how good our trained algorithm is with respect to reality. If metrics are applied on the training data we will know if our algorithm has been able to learn from the data provided. But to obtain reliable and verifiable results about the good performance of the algorithm we will have to test it on data other than those used for training. By default, the sklearn library offers functions to use the most common metrics:\n",
    "\n",
    "<center> <img src=\"img/matcon.jpg\"></center>\n",
    "<center>Fuente: Confusion Matrix - Applied Deep Learning with Keras</center>\n",
    "\n",
    "\n",
    "<center> <img src=\"img/met.png\"></center>\n",
    "<center>Fuente: https://www.researchgate.net/</center>\n",
    "\n",
    "First, let's apply the metrics to the training data to observe which algorithm has learned better, i.e. identified and extracted more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We classify the points (through the features) (This may take a few minutes)\n",
    "\n",
    "# With the SVM\n",
    "train_predictions_SVM = clf_svm.predict(train_features)\n",
    "\n",
    "# With the RF\n",
    "train_predictions_RF = clf_rf.predict(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation of metrics for the SVM algorithm\n",
    "\n",
    "# Here we define the ground truth data.\n",
    "y_test = train_labels\n",
    "\n",
    "# Here we define the data we have calculated.\n",
    "y_pred = train_predictions_SVM\n",
    "\n",
    "# Calculates and displays the confusion matrix\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "\n",
    "# Calculates and displays statistics by class\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "# Calculates and displays global statistics\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo de métricas para el algoritmo RF\n",
    "\n",
    "# Here we define the ground truth data.\n",
    "y_test = train_labels\n",
    "\n",
    "# Here we define the data we have calculated.\n",
    "y_pred = train_predictions_RF\n",
    "\n",
    "# Calculates and displays the confusion matrix\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "\n",
    "# Calculates and displays statistics by class\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "# Calculates and displays global statistics\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen in the metrics above, both classifiers were trained correctly and obtained high success rates, with those of the RF being higher than those of the SVM. At least that is what can be deduced from the predictions with the training data. For a stricter evaluation, we will now use the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data reading\n",
    "test_data = np.loadtxt(\"Nubes/000079.txt\", delimiter=' ')\n",
    "\n",
    "# Separate input data\n",
    "test_coord,test_labels = separar_input(test_data)\n",
    "\n",
    "# Extract features\n",
    "test_features = extraer_features(test_coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We classify the points (through the features) (This may take a few minutes)\n",
    "\n",
    "# SVM\n",
    "test_predictions_SVM = clf_svm.predict(test_features)\n",
    "\n",
    "# RF\n",
    "test_predictions_RF = clf_rf.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation of metrics for the SVM algorithm\n",
    "\n",
    "# Here we define the ground truth data.\n",
    "y_test = test_labels\n",
    "\n",
    "# Here we define the data we have calculated.\n",
    "y_pred = test_predictions_SVM\n",
    "\n",
    "# Calculates and displays the confusion matrix\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "\n",
    "# Calculates and displays statistics by class\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "# Calculates and displays global statistics\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation of metrics for the RF algorithm\n",
    "\n",
    "# Here we define the ground truth data.\n",
    "y_test = test_labels\n",
    "\n",
    "# Here we define the data we have calculated.\n",
    "y_pred = test_predictions_RF\n",
    "\n",
    "# Calculates and displays the confusion matrix\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "\n",
    "# Calculates and displays statistics by class\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "# Calculates and displays global statisticss\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the golbal accuracies are very similar between both classifiers. For this reason it is important to test the results with data independent of training. Comparing the training and testing data, we see that both algorithms have a strong tendency to overfitting, being more noticeable with the RF that goes from an accuracy of 99% to 76%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of the classification\n",
    "\n",
    "An advantage of point clouds is that we can interpret the results based on visualization as well, since they are geometric data. If we use Artificial Intelligence to solve purely numerical problems, such a clear visualization of the data is not possible. The visualization of the data is very relevant to be able to identify possible failures that go unnoticed in the metrics.\n",
    "\n",
    "Finally, we proceed to export the results in a point cloud to visualize in CloudCompare. The exported cloud will have the following columns: \n",
    "- 3 columns of coordinates\n",
    "- 1 column of labels (ground truth)\n",
    "- 1 column of SVM prediction\n",
    "- 1 RF prediction column\n",
    "\n",
    "Are any problems detected that were not detected in the metrics? Are the results as similar as the metrics suggest? Which classes have been classified better? Being able to visualize the problems, can you think of any better solution than the one proposed with the metrics alone?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export\n",
    "# Definition of the path and file name\n",
    "ruta = \"Nubes/00000079_predicted.txt\"\n",
    "\n",
    "#Data selection \n",
    "datos = np.column_stack((test_data,test_predictions_SVM,test_predictions_RF))\n",
    "\n",
    "# Saved\n",
    "np.savetxt(ruta,datos,delimiter=' ') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
